#!/bin/bash

set -e

version=v0.0.1
progname=$(basename $0)
docstr="$progname $version: Stages build artifacts for deployment to AWS"

usage() {
echo "Usage:
  $progname --bucket=bucket --region=region --stack-name=name [--ngap] [--subnet=subnet]

Options:
  --subnet=subnet      The subnet which instances will be deployed into (default: the VPC's default subnet)
  --bucket=bucket      The bucket into which artifacts should be staged
  --region=region      Deploy into the given region
  --stack-name=name    The name of the stack that will be deployed using staged artifacts
  --ngap               The stack is to be deployed to NGAP

Examples:
  $progname --subnet=subnet1234 --bucket=my-bucket --region=us-west-2 --stack-name=gibs-sit"
}

function echoerr() {
    echo "Error: $@" 1>&2;
}

while [[ $# -gt 0 ]]
do

    key="$1"

    if [[ $key == -* ]]; then
        # Dash argument
        case $key in
            --subnet)
                subnet="$2"
                shift
                ;;
            --bucket)
                bucket="$2"
                shift
                ;;
            --region)
                region="$2"
                shift
                ;;
            --stack-name)
                stack_name="$2"
                shift
                ;;
            --ngap)
                ;;
            --help)
                echo "$docstr"
                echo
                usage
                exit 0
                ;;
            *)
                echoerr "Unknown option: $key"
                exit 2
                ;;
        esac
    fi
    shift
done

if [ "${#args[@]}" -ne 0 ]; then
    echo "$docstr"
    echo
    usage
    exit 1
fi

if [ -z $bucket ] || [ -z $region ] || [ -z $stack_name ]; then
    echo "$docstr"
    echo
    usage
    exit 1
fi

# Clean up old artifacts
rm -rf artifacts
mkdir -p artifacts
hashcmd=$(command -v shasum || command -v sha1sum)

function join_by { local IFS="$1"; shift; echo "$*"; }

# For every *-workflows subdirectory, take its compiled assets, zip them up, uniquify names
# and add said name to the "locs" mapping, and upload to S3
locs=()
shopt -s nullglob
for dir in *-workflows; do
    # Work around problem in NGAP deployment where files are zipped with inadequate permissions
    # for Lambda
    cd $dir
    find dist -exec chmod a+r {} \;
    find dist -type d -exec chmod a+x {} \;

    # Iterate over compiled lambdas / assets
    cd dist
    for file in *
    do
        # Hash the contents of the lambda so that we get a unique name if any file changes
        artifact_hash=$(find $file -type f | xargs $hashcmd | $hashcmd | awk '{print $1}')

        # Zip the lambda
        zipfile=../../artifacts/$(basename $file .js).zip
        zip -r $(dirname $zipfile)/$(basename $zipfile .zip) $file

        # Upload to S3
        s3path="lambdas/$file/$artifact_hash.zip"
        aws s3 cp "$zipfile" "s3://$bucket/$s3path"

        # Put a partial JSON object lambda-name: s3-path mapping in the locs array
        locs+=("\"$file\":\"$s3path\"")
    done

    cd ../..
done

# Write the locations mapping to a json file so it can be read by cloudformation to determine the
# unique S3 paths of each Lambda
locs_json=\{$(join_by , "${locs[@]}")\}
echo ${locs_json} > artifacts/locations.json

artifact_locs=artifacts/locations.json stack_name=$stack_name subnet=$subnet erb config/cloudformation.yml.erb > artifacts/cloudformation.yml
aws s3 cp artifacts/cloudformation.yml "s3://$bucket/ingest/cloudformation.yml"

# Copy up the ingest configuration and the compiled cloudformation template
aws s3 cp --recursive config/ "s3://$bucket/ingest/"
